{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mental Health Detection Model\n",
    "\n",
    "This notebook contains a complete pipeline for detecting mental health issues (specifically depression) from text/tweets.\n",
    "\n",
    "## Features:\n",
    "- Text preprocessing pipeline\n",
    "- TF-IDF vectorization\n",
    "- Logistic Regression model (95.6% accuracy)\n",
    "- Prediction function for new tweets\n",
    "- Model saving and loading capabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import pickle\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report, roc_curve, auc, roc_auc_score\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better-looking plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Download NLTK resources (only needed once)\n",
    "try:\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('omw-1.4', quiet=True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Text Preprocessing Function\n",
    "\n",
    "This function cleans and preprocesses text data to prepare it for model training and prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: I'm feeling really down today. Can't stop thinking about negative things. https://example.com\n",
      "Processed: im feeling really today cant stop thinking negative thing\n"
     ]
    }
   ],
   "source": [
    "# Initialize preprocessing components\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess text by:\n",
    "    1. Converting to lowercase\n",
    "    2. Removing URLs\n",
    "    3. Removing HTML tags\n",
    "    4. Removing punctuation\n",
    "    5. Removing numbers and non-alphabetic characters\n",
    "    6. Removing stopwords\n",
    "    7. Lemmatizing words\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to preprocess\n",
    "        \n",
    "    Returns:\n",
    "        str: Preprocessed text\n",
    "    \"\"\"\n",
    "    # Handle NaN and None values\n",
    "    if pd.isna(text) or text is None:\n",
    "        return ''\n",
    "    \n",
    "    text = str(text).lower()                                   # Lowercase\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", '', text)                 # Remove URLs\n",
    "    text = re.sub(r\"<.*?>\", \" \", text)                         # Remove HTML tags\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
    "    words = text.split()\n",
    "    words = [w for w in words if w.isalpha()]                  # Remove numbers & non-alphabetic\n",
    "    words = [w for w in words if w not in stop_words]          # Remove stopwords\n",
    "    words = [lemmatizer.lemmatize(w) for w in words]           # Lemmatize\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Test the preprocessing function\n",
    "test_text = \"I'm feeling really down today. Can't stop thinking about negative things. https://example.com\"\n",
    "print(\"Original:\", test_text)\n",
    "print(\"Processed:\", preprocess_text(test_text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Data Visualization - Label Distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize label distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Count plot\n",
    "label_counts = df['is_depression'].value_counts()\n",
    "axes[0].bar(['No Depression', 'Depression'], label_counts.values, color=['#4caf50', '#f44336'], alpha=0.7)\n",
    "axes[0].set_title('Label Distribution (Count)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "axes[0].set_xlabel('Label', fontsize=12)\n",
    "for i, v in enumerate(label_counts.values):\n",
    "    axes[0].text(i, v + 50, str(v), ha='center', fontweight='bold', fontsize=12)\n",
    "\n",
    "# Pie chart\n",
    "label_props = df['is_depression'].value_counts(normalize=True)\n",
    "colors = ['#4caf50', '#f44336']\n",
    "axes[1].pie(label_props.values, labels=['No Depression', 'Depression'], autopct='%1.1f%%', \n",
    "            colors=colors, startangle=90, textprops={'fontsize': 12, 'fontweight': 'bold'})\n",
    "axes[1].set_title('Label Distribution (Percentage)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Dataset is {'balanced' if abs(label_props[0] - label_props[1]) < 0.1 else 'imbalanced'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Text Length Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate text lengths\n",
    "df['text_length'] = df['clean_text'].apply(lambda x: len(str(x).split()))\n",
    "df['char_length'] = df['clean_text'].apply(lambda x: len(str(x)))\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Word count distribution by label\n",
    "for label in [0, 1]:\n",
    "    label_name = 'No Depression' if label == 0 else 'Depression'\n",
    "    data = df[df['is_depression'] == label]['text_length']\n",
    "    axes[0, 0].hist(data, bins=50, alpha=0.6, label=label_name, \n",
    "                    color='#4caf50' if label == 0 else '#f44336', edgecolor='black')\n",
    "axes[0, 0].set_xlabel('Word Count', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0, 0].set_title('Word Count Distribution by Label', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Box plot comparison\n",
    "df_box = df[['is_depression', 'text_length']].copy()\n",
    "df_box['is_depression'] = df_box['is_depression'].map({0: 'No Depression', 1: 'Depression'})\n",
    "sns.boxplot(data=df_box, x='is_depression', y='text_length', ax=axes[0, 1], \n",
    "            palette=['#4caf50', '#f44336'])\n",
    "axes[0, 1].set_title('Word Count Distribution (Box Plot)', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Label', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Word Count', fontsize=12)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Average text length by label\n",
    "avg_lengths = df.groupby('is_depression')['text_length'].mean()\n",
    "bars = axes[1, 0].bar(['No Depression', 'Depression'], avg_lengths.values, \n",
    "                      color=['#4caf50', '#f44336'], alpha=0.7, edgecolor='black')\n",
    "axes[1, 0].set_title('Average Word Count by Label', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Average Word Count', fontsize=12)\n",
    "axes[1, 0].set_xlabel('Label', fontsize=12)\n",
    "for i, v in enumerate(avg_lengths.values):\n",
    "    axes[1, 0].text(i, v + 2, f'{v:.1f}', ha='center', fontweight='bold', fontsize=12)\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 4. Character length distribution\n",
    "for label in [0, 1]:\n",
    "    label_name = 'No Depression' if label == 0 else 'Depression'\n",
    "    data = df[df['is_depression'] == label]['char_length']\n",
    "    axes[1, 1].hist(data, bins=50, alpha=0.6, label=label_name,\n",
    "                    color='#4caf50' if label == 0 else '#f44336', edgecolor='black')\n",
    "axes[1, 1].set_xlabel('Character Count', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1, 1].set_title('Character Count Distribution by Label', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\n=== Text Length Statistics ===\")\n",
    "print(f\"\\nOverall Statistics:\")\n",
    "print(df['text_length'].describe())\n",
    "print(f\"\\nBy Label:\")\n",
    "print(df.groupby('is_depression')['text_length'].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Word Frequency Analysis & Word Clouds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze most common words by label\n",
    "def get_top_words(texts, n=20):\n",
    "    \"\"\"Get top N words from a list of texts\"\"\"\n",
    "    all_words = []\n",
    "    for text in texts:\n",
    "        words = str(text).lower().split()\n",
    "        all_words.extend(words)\n",
    "    word_freq = Counter(all_words)\n",
    "    return word_freq.most_common(n)\n",
    "\n",
    "# Get top words for each class\n",
    "depression_texts = df[df['is_depression'] == 1]['processed_text']\n",
    "no_depression_texts = df[df['is_depression'] == 0]['processed_text']\n",
    "\n",
    "top_depression_words = get_top_words(depression_texts, 20)\n",
    "top_no_depression_words = get_top_words(no_depression_texts, 20)\n",
    "\n",
    "# Visualize top words\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Depression words\n",
    "dep_words, dep_counts = zip(*top_depression_words)\n",
    "axes[0].barh(range(len(dep_words)), dep_counts, color='#f44336', alpha=0.7)\n",
    "axes[0].set_yticks(range(len(dep_words)))\n",
    "axes[0].set_yticklabels(dep_words, fontsize=10)\n",
    "axes[0].set_xlabel('Frequency', fontsize=12)\n",
    "axes[0].set_title('Top 20 Words - Depression Class', fontsize=14, fontweight='bold')\n",
    "axes[0].invert_yaxis()\n",
    "axes[0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# No Depression words\n",
    "no_dep_words, no_dep_counts = zip(*top_no_depression_words)\n",
    "axes[1].barh(range(len(no_dep_words)), no_dep_counts, color='#4caf50', alpha=0.7)\n",
    "axes[1].set_yticks(range(len(no_dep_words)))\n",
    "axes[1].set_yticklabels(no_dep_words, fontsize=10)\n",
    "axes[1].set_xlabel('Frequency', fontsize=12)\n",
    "axes[1].set_title('Top 20 Words - No Depression Class', fontsize=14, fontweight='bold')\n",
    "axes[1].invert_yaxis()\n",
    "axes[1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Word Clouds\n",
    "print(\"\\nGenerating Word Clouds...\")\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# Depression word cloud\n",
    "depression_text = ' '.join(depression_texts.astype(str))\n",
    "wordcloud_dep = WordCloud(width=800, height=400, background_color='white', \n",
    "                          colormap='Reds', max_words=100).generate(depression_text)\n",
    "axes[0].imshow(wordcloud_dep, interpolation='bilinear')\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title('Word Cloud - Depression Class', fontsize=16, fontweight='bold', pad=20)\n",
    "\n",
    "# No Depression word cloud\n",
    "no_depression_text = ' '.join(no_depression_texts.astype(str))\n",
    "wordcloud_no_dep = WordCloud(width=800, height=400, background_color='white', \n",
    "                            colormap='Greens', max_words=100).generate(no_depression_text)\n",
    "axes[1].imshow(wordcloud_no_dep, interpolation='bilinear')\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title('Word Cloud - No Depression Class', fontsize=16, fontweight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. TF-IDF Feature Matrix Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize TF-IDF feature statistics\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Calculate feature statistics\n",
    "feature_means = np.array(X_tfidf.mean(axis=0)).flatten()\n",
    "feature_stds = np.array(X_tfidf.std(axis=0)).flatten()\n",
    "\n",
    "# 1. Feature mean distribution\n",
    "axes[0].hist(feature_means, bins=50, color='#2196F3', alpha=0.7, edgecolor='black')\n",
    "axes[0].set_xlabel('Mean TF-IDF Value', fontsize=12)\n",
    "axes[0].set_ylabel('Number of Features', fontsize=12)\n",
    "axes[0].set_title('Distribution of Feature Means', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "axes[0].axvline(x=np.mean(feature_means), color='red', linestyle='--', linewidth=2, \n",
    "                label=f'Overall Mean: {np.mean(feature_means):.4f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# 2. Feature standard deviation distribution\n",
    "axes[1].hist(feature_stds, bins=50, color='#ff9800', alpha=0.7, edgecolor='black')\n",
    "axes[1].set_xlabel('Standard Deviation of TF-IDF Values', fontsize=12)\n",
    "axes[1].set_ylabel('Number of Features', fontsize=12)\n",
    "axes[1].set_title('Distribution of Feature Standard Deviations', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "axes[1].axvline(x=np.mean(feature_stds), color='red', linestyle='--', linewidth=2,\n",
    "                label=f'Overall Mean: {np.mean(feature_stds):.4f}')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n=== TF-IDF Feature Statistics ===\")\n",
    "print(f\"Total features: {X_tfidf.shape[1]}\")\n",
    "print(f\"Mean TF-IDF value: {np.mean(feature_means):.4f}\")\n",
    "print(f\"Std TF-IDF value: {np.mean(feature_stds):.4f}\")\n",
    "print(f\"Max TF-IDF value: {X_tfidf.max():.4f}\")\n",
    "print(f\"Min TF-IDF value: {X_tfidf.min():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Train-Test Split Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize train-test split\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Train set distribution\n",
    "train_counts = pd.Series(y_train).value_counts()\n",
    "axes[0].bar(['No Depression', 'Depression'], train_counts.values, \n",
    "            color=['#4caf50', '#f44336'], alpha=0.7, edgecolor='black')\n",
    "axes[0].set_title(f'Train Set Distribution (n={len(y_train)})', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "axes[0].set_xlabel('Label', fontsize=12)\n",
    "for i, v in enumerate(train_counts.values):\n",
    "    axes[0].text(i, v + 20, f'{v}\\n({v/len(y_train)*100:.1f}%)', \n",
    "                ha='center', fontweight='bold', fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Test set distribution\n",
    "test_counts = pd.Series(y_test).value_counts()\n",
    "axes[1].bar(['No Depression', 'Depression'], test_counts.values, \n",
    "            color=['#4caf50', '#f44336'], alpha=0.7, edgecolor='black')\n",
    "axes[1].set_title(f'Test Set Distribution (n={len(y_test)})', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Count', fontsize=12)\n",
    "axes[1].set_xlabel('Label', fontsize=12)\n",
    "for i, v in enumerate(test_counts.values):\n",
    "    axes[1].text(i, v + 5, f'{v}\\n({v/len(y_test)*100:.1f}%)', \n",
    "                ha='center', fontweight='bold', fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n=== Train-Test Split Summary ===\")\n",
    "print(f\"Training samples: {len(y_train)} ({len(y_train)/len(y)*100:.1f}%)\")\n",
    "print(f\"Test samples: {len(y_test)} ({len(y_test)/len(y)*100:.1f}%)\")\n",
    "print(f\"\\nTrain set is balanced: {abs(train_counts[0]/len(y_train) - train_counts[1]/len(y_train)) < 0.05}\")\n",
    "print(f\"Test set is balanced: {abs(test_counts[0]/len(y_test) - test_counts[1]/len(y_test)) < 0.05}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Model Performance Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive model performance visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Confusion Matrix (Heatmap)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 0],\n",
    "            xticklabels=['No Depression', 'Depression'],\n",
    "            yticklabels=['No Depression', 'Depression'],\n",
    "            cbar_kws={'label': 'Count'})\n",
    "axes[0, 0].set_title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('True Label', fontsize=12)\n",
    "axes[0, 0].set_xlabel('Predicted Label', fontsize=12)\n",
    "\n",
    "# Add percentages\n",
    "cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        axes[0, 0].text(j+0.5, i+0.7, f'{cm_percent[i, j]:.1f}%', \n",
    "                       ha='center', va='center', fontsize=10, color='red', fontweight='bold')\n",
    "\n",
    "# 2. ROC Curve\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "axes[0, 1].plot(fpr, tpr, color='#2196F3', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "axes[0, 1].plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--', label='Random Classifier')\n",
    "axes[0, 1].set_xlim([0.0, 1.0])\n",
    "axes[0, 1].set_ylim([0.0, 1.05])\n",
    "axes[0, 1].set_xlabel('False Positive Rate', fontsize=12)\n",
    "axes[0, 1].set_ylabel('True Positive Rate', fontsize=12)\n",
    "axes[0, 1].set_title('ROC Curve', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].legend(loc=\"lower right\")\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Metrics Bar Chart\n",
    "metrics = {\n",
    "    'Accuracy': accuracy,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': f1\n",
    "}\n",
    "bars = axes[1, 0].bar(metrics.keys(), metrics.values(), color=['#4caf50', '#2196F3', '#ff9800', '#9c27b0'], alpha=0.7, edgecolor='black')\n",
    "axes[1, 0].set_ylim([0, 1])\n",
    "axes[1, 0].set_ylabel('Score', fontsize=12)\n",
    "axes[1, 0].set_title('Model Performance Metrics', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "for i, (key, value) in enumerate(metrics.items()):\n",
    "    axes[1, 0].text(i, value + 0.02, f'{value:.3f}', ha='center', fontweight='bold', fontsize=11)\n",
    "\n",
    "# 4. Prediction Probability Distribution\n",
    "y_pred_proba_depression = y_pred_proba\n",
    "y_pred_proba_no_depression = 1 - y_pred_proba\n",
    "\n",
    "axes[1, 1].hist(y_pred_proba_no_depression[y_test == 0], bins=30, alpha=0.6, \n",
    "                label='No Depression (True)', color='#4caf50', edgecolor='black')\n",
    "axes[1, 1].hist(y_pred_proba_depression[y_test == 1], bins=30, alpha=0.6, \n",
    "                label='Depression (True)', color='#f44336', edgecolor='black')\n",
    "axes[1, 1].axvline(x=0.5, color='black', linestyle='--', linewidth=2, label='Decision Threshold (0.5)')\n",
    "axes[1, 1].set_xlabel('Predicted Probability', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1, 1].set_title('Prediction Probability Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n=== Model Performance Summary ===\")\n",
    "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Feature Importance Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from Logistic Regression model\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "coefficients = model.coef_[0]\n",
    "\n",
    "# Get top features for depression (positive coefficients)\n",
    "top_depression_features = sorted(zip(feature_names, coefficients), key=lambda x: x[1], reverse=True)[:20]\n",
    "# Get top features for no depression (negative coefficients)\n",
    "top_no_depression_features = sorted(zip(feature_names, coefficients), key=lambda x: x[1])[:20]\n",
    "\n",
    "# Visualize feature importance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 10))\n",
    "\n",
    "# Top features indicating depression\n",
    "dep_features, dep_coeffs = zip(*top_depression_features)\n",
    "axes[0].barh(range(len(dep_features)), dep_coeffs, color='#f44336', alpha=0.7)\n",
    "axes[0].set_yticks(range(len(dep_features)))\n",
    "axes[0].set_yticklabels(dep_features, fontsize=10)\n",
    "axes[0].set_xlabel('Coefficient Value', fontsize=12)\n",
    "axes[0].set_title('Top 20 Features - Depression Indicators', fontsize=14, fontweight='bold')\n",
    "axes[0].invert_yaxis()\n",
    "axes[0].grid(True, alpha=0.3, axis='x')\n",
    "axes[0].axvline(x=0, color='black', linestyle='-', linewidth=1)\n",
    "\n",
    "# Top features indicating no depression\n",
    "no_dep_features, no_dep_coeffs = zip(*top_no_depression_features)\n",
    "axes[1].barh(range(len(no_dep_features)), no_dep_coeffs, color='#4caf50', alpha=0.7)\n",
    "axes[1].set_yticks(range(len(no_dep_features)))\n",
    "axes[1].set_yticklabels(no_dep_features, fontsize=10)\n",
    "axes[1].set_xlabel('Coefficient Value', fontsize=12)\n",
    "axes[1].set_title('Top 20 Features - No Depression Indicators', fontsize=14, fontweight='bold')\n",
    "axes[1].invert_yaxis()\n",
    "axes[1].grid(True, alpha=0.3, axis='x')\n",
    "axes[1].axvline(x=0, color='black', linestyle='-', linewidth=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== Top Features for Depression Detection ===\")\n",
    "print(\"Positive coefficients (indicate depression):\")\n",
    "for feature, coeff in top_depression_features[:10]:\n",
    "    print(f\"  {feature}: {coeff:.4f}\")\n",
    "    \n",
    "print(\"\\nNegative coefficients (indicate no depression):\")\n",
    "for feature, coeff in top_no_depression_features[:10]:\n",
    "    print(f\"  {feature}: {coeff:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Error Analysis - Misclassified Samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze misclassified samples\n",
    "# Get test indices from the split - recreate the split to get indices\n",
    "_, test_indices = train_test_split(range(len(df)), test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Get original text for misclassified samples\n",
    "test_df = df.iloc[test_indices].copy()\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "# Add predictions\n",
    "test_df['predicted'] = y_pred\n",
    "test_df['actual'] = y_test\n",
    "test_df['predicted_proba'] = y_pred_proba\n",
    "\n",
    "# False Positives (predicted depression but actually no depression)\n",
    "false_positives = test_df[(test_df['predicted'] == 1) & (test_df['actual'] == 0)]\n",
    "# False Negatives (predicted no depression but actually depression)\n",
    "false_negatives = test_df[(test_df['predicted'] == 0) & (test_df['actual'] == 1)]\n",
    "\n",
    "print(f\"=== Error Analysis ===\")\n",
    "print(f\"\\nFalse Positives (Predicted: Depression, Actual: No Depression): {len(false_positives)}\")\n",
    "print(f\"False Negatives (Predicted: No Depression, Actual: Depression): {len(false_negatives)}\")\n",
    "\n",
    "# Visualize error distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# False Positives probability distribution\n",
    "if len(false_positives) > 0:\n",
    "    axes[0].hist(false_positives['predicted_proba'], bins=20, color='#ff9800', alpha=0.7, edgecolor='black')\n",
    "    axes[0].set_xlabel('Predicted Depression Probability', fontsize=12)\n",
    "    axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "    axes[0].set_title(f'False Positives Distribution (n={len(false_positives)})', fontsize=14, fontweight='bold')\n",
    "    axes[0].axvline(x=0.5, color='red', linestyle='--', linewidth=2, label='Decision Threshold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# False Negatives probability distribution\n",
    "if len(false_negatives) > 0:\n",
    "    axes[1].hist(false_negatives['predicted_proba'], bins=20, color='#9c27b0', alpha=0.7, edgecolor='black')\n",
    "    axes[1].set_xlabel('Predicted Depression Probability', fontsize=12)\n",
    "    axes[1].set_ylabel('Frequency', fontsize=12)\n",
    "    axes[1].set_title(f'False Negatives Distribution (n={len(false_negatives)})', fontsize=14, fontweight='bold')\n",
    "    axes[1].axvline(x=0.5, color='red', linestyle='--', linewidth=2, label='Decision Threshold')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show sample misclassified texts\n",
    "print(\"\\n=== Sample False Positives (Predicted Depression, Actually No Depression) ===\")\n",
    "for idx, row in false_positives.head(5).iterrows():\n",
    "    print(f\"\\nText: {row['clean_text'][:200]}...\")\n",
    "    print(f\"Predicted Probability: {row['predicted_proba']:.3f}\")\n",
    "\n",
    "print(\"\\n=== Sample False Negatives (Predicted No Depression, Actually Depression) ===\")\n",
    "for idx, row in false_negatives.head(5).iterrows():\n",
    "    print(f\"\\nText: {row['clean_text'][:200]}...\")\n",
    "    print(f\"Predicted Probability: {row['predicted_proba']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1. Prediction Visualization Examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions for sample tweets\n",
    "test_tweets = [\n",
    "    \"I'm feeling really down today. Can't stop thinking about negative things. Life feels meaningless.\",\n",
    "    \"Had a great day today! Went for a walk and met some friends. Feeling happy and energized!\",\n",
    "    \"I don't want to get out of bed. Everything feels hopeless and I can't see a way out.\",\n",
    "    \"Just finished a productive day at work. Looking forward to the weekend!\",\n",
    "    \"I've been having suicidal thoughts lately. I don't know what to do anymore.\"\n",
    "]\n",
    "\n",
    "# Get predictions\n",
    "results = []\n",
    "for tweet in test_tweets:\n",
    "    result = predict_mental_health(tweet, model=model, vectorizer=vectorizer, return_probability=True, depression_threshold=0.40)\n",
    "    results.append(result)\n",
    "\n",
    "# Visualize predictions\n",
    "fig, axes = plt.subplots(len(results), 1, figsize=(14, 3*len(results)))\n",
    "if len(results) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i, (tweet, result) in enumerate(zip(test_tweets, results)):\n",
    "    probs = result['probabilities']\n",
    "    colors = ['#4caf50' if result['prediction'] == 0 else '#f44336', \n",
    "              '#f44336' if result['prediction'] == 1 else '#4caf50']\n",
    "    \n",
    "    bars = axes[i].bar(['No Depression', 'Depression'], \n",
    "                       [probs['No depression'], probs['Depression']], \n",
    "                       color=colors, alpha=0.7, edgecolor='black')\n",
    "    axes[i].set_ylabel('Probability (%)', fontsize=11)\n",
    "    axes[i].set_ylim([0, 100])\n",
    "    axes[i].set_title(f\"Text: {tweet[:80]}...\", fontsize=11, fontweight='bold', pad=10)\n",
    "    axes[i].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for j, (label, prob) in enumerate(probs.items()):\n",
    "        axes[i].text(j, prob + 2, f'{prob}%', ha='center', fontweight='bold', fontsize=10)\n",
    "    \n",
    "    # Add prediction label\n",
    "    pred_label = result['label']\n",
    "    pred_color = '#f44336' if result['prediction'] == 1 else '#4caf50'\n",
    "    axes[i].text(0.5, 90, pred_label, ha='center', fontsize=12, fontweight='bold', \n",
    "                color=pred_color, bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed results\n",
    "print(\"\\n=== Detailed Prediction Results ===\")\n",
    "for i, (tweet, result) in enumerate(zip(test_tweets, results), 1):\n",
    "    print(f\"\\n{i}. {tweet[:100]}...\")\n",
    "    print(f\"   Prediction: {result['label']}\")\n",
    "    print(f\"   Depression Probability: {result['depression_probability']}%\")\n",
    "    print(f\"   Confidence: {result['confidence']}%\")\n",
    "    print(f\"   Probabilities: {result['probabilities']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded processed dataset: (7731, 4)\n",
      "Using existing processed_text column\n",
      "\n",
      "Checking for missing values:\n",
      "Missing values in clean_text: 0\n",
      "Missing values in processed_text: 1\n",
      "Missing values in is_depression: 0\n",
      "\n",
      "Filling NaN values in processed_text...\n",
      "\n",
      "Removed 1 rows with empty or invalid processed text\n",
      "\n",
      "Dataset shape: (7730, 4)\n",
      "Columns: ['clean_text', 'is_depression', 'text_length', 'processed_text']\n",
      "\n",
      "Label distribution:\n",
      "is_depression\n",
      "0    3900\n",
      "1    3830\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Label proportions:\n",
      "is_depression\n",
      "0    0.504528\n",
      "1    0.495472\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>is_depression</th>\n",
       "      <th>text_length</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>we understand that most people who reply immed...</td>\n",
       "      <td>1</td>\n",
       "      <td>813</td>\n",
       "      <td>understand people reply immediately op invitat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>welcome to r depression s check in post a plac...</td>\n",
       "      <td>1</td>\n",
       "      <td>429</td>\n",
       "      <td>welcome r depression check post place take mom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anyone else instead of sleeping more when depr...</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>anyone else instead sleeping depressed stay ni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i ve kind of stuffed around a lot in my life d...</td>\n",
       "      <td>1</td>\n",
       "      <td>110</td>\n",
       "      <td>kind stuffed around lot life delaying inevitab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sleep is my greatest and most comforting escap...</td>\n",
       "      <td>1</td>\n",
       "      <td>54</td>\n",
       "      <td>sleep greatest comforting escape whenever wake...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_text  is_depression  \\\n",
       "0  we understand that most people who reply immed...              1   \n",
       "1  welcome to r depression s check in post a plac...              1   \n",
       "2  anyone else instead of sleeping more when depr...              1   \n",
       "3  i ve kind of stuffed around a lot in my life d...              1   \n",
       "4  sleep is my greatest and most comforting escap...              1   \n",
       "\n",
       "   text_length                                     processed_text  \n",
       "0          813  understand people reply immediately op invitat...  \n",
       "1          429  welcome r depression check post place take mom...  \n",
       "2           45  anyone else instead sleeping depressed stay ni...  \n",
       "3          110  kind stuffed around lot life delaying inevitab...  \n",
       "4           54  sleep greatest comforting escape whenever wake...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set base directory (adjust if needed)\n",
    "base_dir = 'D:/mental_health_detector'  # Change this to your project path if different\n",
    "\n",
    "# Load the processed dataset\n",
    "data_path = os.path.join(base_dir, 'data/processed/depression_dataset_processed.csv')\n",
    "raw_data_path = os.path.join(base_dir, 'data/raw/depression_dataset_reddit_cleaned.csv')\n",
    "\n",
    "# Check if processed data exists, otherwise use raw data\n",
    "if os.path.exists(data_path):\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"Loaded processed dataset: {df.shape}\")\n",
    "    # If processed_text column exists, use it; otherwise process clean_text\n",
    "    if 'processed_text' in df.columns:\n",
    "        print(\"Using existing processed_text column\")\n",
    "    else:\n",
    "        print(\"Processing clean_text column...\")\n",
    "        df['processed_text'] = df['clean_text'].apply(preprocess_text)\n",
    "else:\n",
    "    # Load raw data and process it\n",
    "    print(f\"Processed data not found. Loading raw data from {raw_data_path}\")\n",
    "    df = pd.read_csv(raw_data_path)\n",
    "    print(f\"Raw dataset shape: {df.shape}\")\n",
    "    df['processed_text'] = df['clean_text'].apply(preprocess_text)\n",
    "\n",
    "# Clean the data - handle NaN values\n",
    "print(f\"\\nChecking for missing values:\")\n",
    "print(f\"Missing values in clean_text: {df['clean_text'].isna().sum()}\")\n",
    "print(f\"Missing values in processed_text: {df['processed_text'].isna().sum()}\")\n",
    "print(f\"Missing values in is_depression: {df['is_depression'].isna().sum()}\")\n",
    "\n",
    "# Fill NaN values in processed_text with empty string or reprocess clean_text\n",
    "if df['processed_text'].isna().sum() > 0:\n",
    "    print(\"\\nFilling NaN values in processed_text...\")\n",
    "    # If processed_text has NaN, try to reprocess from clean_text\n",
    "    mask = df['processed_text'].isna()\n",
    "    df.loc[mask, 'processed_text'] = df.loc[mask, 'clean_text'].apply(preprocess_text)\n",
    "    \n",
    "# Remove rows where processed_text is still empty or NaN after processing\n",
    "initial_shape = df.shape[0]\n",
    "df = df[df['processed_text'].notna() & (df['processed_text'].str.strip() != '')]\n",
    "df = df[df['is_depression'].notna()]  # Also remove rows with missing labels\n",
    "final_shape = df.shape[0]\n",
    "\n",
    "if initial_shape != final_shape:\n",
    "    print(f\"\\nRemoved {initial_shape - final_shape} rows with empty or invalid processed text\")\n",
    "\n",
    "# Display dataset info\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(df['is_depression'].value_counts())\n",
    "print(f\"\\nLabel proportions:\")\n",
    "print(df['is_depression'].value_counts(normalize=True))\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Extraction with TF-IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final check for NaN values before vectorization:\n",
      "NaN count in processed_text: 0\n",
      "Empty string count: 0\n",
      "\n",
      "Fitting TF-IDF vectorizer...\n",
      "\n",
      "TF-IDF matrix shape: (7730, 5000)\n",
      "Number of features: 5000\n",
      "\n",
      "Sample feature names: ['aa' 'ab' 'abandoned' 'abandonment' 'ability' 'able' 'able find'\n",
      " 'able get' 'able go' 'absence' 'absolute' 'absolutely'\n",
      " 'absolutely nothing' 'abt' 'abuse' 'abused' 'abusive'\n",
      " 'abusive relationship' 'academic' 'accept']\n"
     ]
    }
   ],
   "source": [
    "# Create TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "\n",
    "# Ensure processed_text has no NaN values before vectorization\n",
    "print(\"Final check for NaN values before vectorization:\")\n",
    "print(f\"NaN count in processed_text: {df['processed_text'].isna().sum()}\")\n",
    "print(f\"Empty string count: {(df['processed_text'].str.strip() == '').sum()}\")\n",
    "\n",
    "# Convert to list and ensure all are strings (no NaN)\n",
    "processed_texts = df['processed_text'].fillna('').astype(str).tolist()\n",
    "\n",
    "# Fit and transform the processed text\n",
    "print(\"\\nFitting TF-IDF vectorizer...\")\n",
    "X_tfidf = vectorizer.fit_transform(processed_texts)\n",
    "y = df['is_depression'].values\n",
    "\n",
    "print(f\"\\nTF-IDF matrix shape: {X_tfidf.shape}\")\n",
    "print(f\"Number of features: {len(vectorizer.get_feature_names_out())}\")\n",
    "print(f\"\\nSample feature names: {vectorizer.get_feature_names_out()[:20]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Split Data into Train and Test Sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_tfidf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Split data: 80% training, 20% testing\u001b[39;00m\n\u001b[0;32m      2\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[1;32m----> 3\u001b[0m     \u001b[43mX_tfidf\u001b[49m, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, stratify\u001b[38;5;241m=\u001b[39my\n\u001b[0;32m      4\u001b[0m )\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_tfidf' is not defined"
     ]
    }
   ],
   "source": [
    "# Split data: 80% training, 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_tfidf, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Test samples: {X_test.shape[0]}\")\n",
    "print(f\"\\nLabel distribution in train:\")\n",
    "train_dist = pd.Series(y_train).value_counts(normalize=True).to_dict()\n",
    "print(train_dist)\n",
    "print(f\"\\nLabel distribution in test:\")\n",
    "test_dist = pd.Series(y_test).value_counts(normalize=True).to_dict()\n",
    "print(test_dist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Model training completed!\n",
      "\n",
      "=== Model Performance ===\n",
      "Accuracy: 0.9547\n",
      "Precision: 0.9780\n",
      "Recall: 0.9295\n",
      "F1-Score: 0.9531\n",
      "\n",
      "=== Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.96       780\n",
      "           1       0.98      0.93      0.95       766\n",
      "\n",
      "    accuracy                           0.95      1546\n",
      "   macro avg       0.96      0.95      0.95      1546\n",
      "weighted avg       0.96      0.95      0.95      1546\n",
      "\n",
      "\n",
      "=== Confusion Matrix ===\n",
      "[[764  16]\n",
      " [ 54 712]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train Logistic Regression model\n",
    "# Using class_weight='balanced' to handle any class imbalance better\n",
    "model = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')\n",
    "print(\"Training model...\")\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Model training completed!\")\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
    "\n",
    "print(f\"\\n=== Model Performance ===\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "print(f\"\\n=== Classification Report ===\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(f\"\\n=== Confusion Matrix ===\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Model and Vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to D:/mental_health_detector\\models\\mental_health_model.pkl\n",
      "Vectorizer saved to D:/mental_health_detector\\models\\tfidf_vectorizer.pkl\n",
      "\n",
      "Model and vectorizer saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Set base directory (should match the one used above)\n",
    "base_dir = 'D:/mental_health_detector'  # Change this to your project path if different\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "models_dir = os.path.join(base_dir, 'models')\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Save the model\n",
    "model_path = os.path.join(models_dir, 'mental_health_model.pkl')\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "print(f\"Model saved to {model_path}\")\n",
    "\n",
    "# Save the vectorizer\n",
    "vectorizer_path = os.path.join(models_dir, 'tfidf_vectorizer.pkl')\n",
    "with open(vectorizer_path, 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "print(f\"Vectorizer saved to {vectorizer_path}\")\n",
    "\n",
    "# Save preprocessing function info (we'll recreate it in prediction)\n",
    "print(\"\\nModel and vectorizer saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Prediction Function for New Tweets\n",
    "\n",
    "This function can be used to predict mental health status from a new tweet or text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing Prediction Function (with 40% depression threshold) ===\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 91\u001b[39m\n\u001b[32m     82\u001b[39m test_tweets = [\n\u001b[32m     83\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mI\u001b[39m\u001b[33m'\u001b[39m\u001b[33mm feeling really down today. Can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt stop thinking about negative things. Life feels meaningless.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     84\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mHad a great day today! Went for a walk and met some friends. Feeling happy and energized!\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     87\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mI\u001b[39m\u001b[33m'\u001b[39m\u001b[33mve been having suicidal thoughts lately. I don\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt know what to do anymore.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     88\u001b[39m ]\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m tweet \u001b[38;5;129;01min\u001b[39;00m test_tweets:\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     result = \u001b[43mpredict_mental_health\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtweet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_probability\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepression_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.40\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTweet: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtweet[:\u001b[32m80\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPrediction: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mpredict_mental_health\u001b[39m\u001b[34m(text, model, vectorizer, return_probability, base_dir, depression_threshold)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Load model and vectorizer if not provided\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     model_path = \u001b[43mos\u001b[49m.path.join(base_dir, \u001b[33m'\u001b[39m\u001b[33mmodels/mental_health_model.pkl\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(model_path, \u001b[33m'\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     21\u001b[39m         model = pickle.load(f)\n",
      "\u001b[31mNameError\u001b[39m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "def predict_mental_health(text, model=None, vectorizer=None, return_probability=False, \n",
    "                          base_dir='D:/mental_health_detector', depression_threshold=0.40):\n",
    "    \"\"\"\n",
    "    Predict mental health status from a text/tweet.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text/tweet to analyze\n",
    "        model: Trained model (if None, loads from saved file)\n",
    "        vectorizer: Trained vectorizer (if None, loads from saved file)\n",
    "        return_probability (bool): If True, returns probability scores\n",
    "        base_dir (str): Base directory path for loading saved models\n",
    "        depression_threshold (float): Threshold for depression detection (default 0.40 = 40%)\n",
    "        \n",
    "    Returns:\n",
    "        dict: Prediction results with label and confidence\n",
    "    \"\"\"\n",
    "    # Load model and vectorizer if not provided\n",
    "    if model is None:\n",
    "        model_path = os.path.join(base_dir, 'models/mental_health_model.pkl')\n",
    "        with open(model_path, 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "    \n",
    "    if vectorizer is None:\n",
    "        vectorizer_path = os.path.join(base_dir, 'models/tfidf_vectorizer.pkl')\n",
    "        with open(vectorizer_path, 'rb') as f:\n",
    "            vectorizer = pickle.load(f)\n",
    "    \n",
    "    # Preprocess the text\n",
    "    processed_text = preprocess_text(text)\n",
    "    \n",
    "    # Handle empty processed text\n",
    "    if not processed_text or processed_text.strip() == '':\n",
    "        return {\n",
    "            'text': text,\n",
    "            'prediction': -1,\n",
    "            'label': 'Insufficient text for analysis',\n",
    "            'confidence': 0.0,\n",
    "            'probabilities': {'No depression': 50.0, 'Depression': 50.0}\n",
    "        }\n",
    "    \n",
    "    # Transform to TF-IDF features\n",
    "    text_tfidf = vectorizer.transform([processed_text])\n",
    "    \n",
    "    # Get probabilities\n",
    "    probability = model.predict_proba(text_tfidf)[0]\n",
    "    depression_prob = probability[1]  # Probability of depression (class 1)\n",
    "    \n",
    "    # Use threshold-based approach for better sensitivity\n",
    "    # If depression probability is above threshold, flag it\n",
    "    if depression_prob >= depression_threshold:\n",
    "        prediction = 1\n",
    "        label = \"Depression detected\"\n",
    "        confidence = depression_prob * 100\n",
    "    else:\n",
    "        prediction = 0\n",
    "        label = \"No depression detected\"\n",
    "        confidence = probability[0] * 100\n",
    "    \n",
    "    # Check if it's a borderline case\n",
    "    is_borderline = abs(depression_prob - 0.5) < 0.15  # Within 15% of 50/50\n",
    "    \n",
    "    result = {\n",
    "        'text': text,\n",
    "        'prediction': int(prediction),\n",
    "        'label': label,\n",
    "        'confidence': round(confidence, 2),\n",
    "        'depression_probability': round(depression_prob * 100, 2),\n",
    "        'is_borderline': is_borderline\n",
    "    }\n",
    "    \n",
    "    if return_probability:\n",
    "        result['probabilities'] = {\n",
    "            'No depression': round(probability[0] * 100, 2),\n",
    "            'Depression': round(probability[1] * 100, 2)\n",
    "        }\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test the prediction function with sample tweets\n",
    "print(\"=== Testing Prediction Function (with 40% depression threshold) ===\\n\")\n",
    "\n",
    "test_tweets = [\n",
    "    \"I'm feeling really down today. Can't stop thinking about negative things. Life feels meaningless.\",\n",
    "    \"Had a great day today! Went for a walk and met some friends. Feeling happy and energized!\",\n",
    "    \"I don't want to get out of bed. Everything feels hopeless and I can't see a way out.\",\n",
    "    \"Just finished a productive day at work. Looking forward to the weekend!\",\n",
    "    \"I've been having suicidal thoughts lately. I don't know what to do anymore.\"\n",
    "]\n",
    "\n",
    "for tweet in test_tweets:\n",
    "    result = predict_mental_health(tweet, return_probability=True, depression_threshold=0.40)\n",
    "    print(f\"Tweet: {tweet[:80]}...\")\n",
    "    print(f\"Prediction: {result['label']}\")\n",
    "    print(f\"Depression Probability: {result['depression_probability']}%\")\n",
    "    print(f\"Confidence: {result['confidence']}%\")\n",
    "    if result.get('is_borderline', False):\n",
    "        print(f\"  Borderline case - model is uncertain\")\n",
    "    print(f\"Full Probabilities: {result['probabilities']}\")\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Interactive Prediction\n",
    "\n",
    "Use this cell to test your own tweets or text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MENTAL HEALTH DETECTION RESULT\n",
      "================================================================================\n",
      "\n",
      "Input Text: I'm feeling great today! Everything is going well.\n",
      "\n",
      "Prediction: No depression detected\n",
      "Confidence: 77.0%\n",
      "\n",
      "Detailed Probabilities:\n",
      "  No depression: 77.0%\n",
      "  Depression: 23.0%\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Enter your tweet/text here\n",
    "your_tweet = \"I'm feeling great today! Everything is going well.\"\n",
    "\n",
    "# Make prediction (using 40% threshold for better sensitivity)\n",
    "result = predict_mental_health(your_tweet, return_probability=True, depression_threshold=0.40)\n",
    "\n",
    "# Display results\n",
    "print(\"=\" * 80)\n",
    "print(\"MENTAL HEALTH DETECTION RESULT\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nInput Text: {result['text']}\")\n",
    "print(f\"\\nPrediction: {result['label']}\")\n",
    "print(f\"Depression Probability: {result['depression_probability']}%\")\n",
    "print(f\"Confidence: {result['confidence']}%\")\n",
    "if result.get('is_borderline', False):\n",
    "    print(f\"\\n  Borderline case - model is uncertain\")\n",
    "print(f\"\\nDetailed Probabilities:\")\n",
    "for label, prob in result['probabilities'].items():\n",
    "    print(f\"  {label}: {prob}%\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Batch Prediction Function\n",
    "\n",
    "For predicting multiple tweets at once.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Batch Prediction Results ===\n",
      "\n",
      "1. Feeling really sad and hopeless today\n",
      "    No depression detected (Confidence: 89.14%)\n",
      "\n",
      "2. Great weather today! Going to the park\n",
      "    No depression detected (Confidence: 94.94%)\n",
      "\n",
      "3. I can't find motivation to do anything\n",
      "    No depression detected (Confidence: 81.98%)\n",
      "\n",
      "4. Excited about my new project!\n",
      "    No depression detected (Confidence: 93.65%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def predict_batch(texts, model=None, vectorizer=None, base_dir='D:/mental_health_detector', \n",
    "                depression_threshold=0.40):\n",
    "    \"\"\"\n",
    "    Predict mental health status for multiple texts.\n",
    "    \n",
    "    Args:\n",
    "        texts (list): List of texts/tweets to analyze\n",
    "        model: Trained model (if None, loads from saved file)\n",
    "        vectorizer: Trained vectorizer (if None, loads from saved file)\n",
    "        base_dir (str): Base directory path for loading saved models\n",
    "        depression_threshold (float): Threshold for depression detection (default 0.40 = 40%)\n",
    "        \n",
    "    Returns:\n",
    "        list: List of prediction results\n",
    "    \"\"\"\n",
    "    # Load model and vectorizer if not provided\n",
    "    if model is None:\n",
    "        model_path = os.path.join(base_dir, 'models/mental_health_model.pkl')\n",
    "        with open(model_path, 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "    \n",
    "    if vectorizer is None:\n",
    "        vectorizer_path = os.path.join(base_dir, 'models/tfidf_vectorizer.pkl')\n",
    "        with open(vectorizer_path, 'rb') as f:\n",
    "            vectorizer = pickle.load(f)\n",
    "    \n",
    "    # Preprocess all texts\n",
    "    processed_texts = [preprocess_text(text) for text in texts]\n",
    "    \n",
    "    # Filter out empty texts and keep track of indices\n",
    "    valid_indices = [i for i, text in enumerate(processed_texts) if text and text.strip() != '']\n",
    "    valid_texts = [processed_texts[i] for i in valid_indices]\n",
    "    \n",
    "    if not valid_texts:\n",
    "        return []\n",
    "    \n",
    "    # Transform to TF-IDF features\n",
    "    texts_tfidf = vectorizer.transform(valid_texts)\n",
    "    \n",
    "    # Get probabilities\n",
    "    probabilities = model.predict_proba(texts_tfidf)\n",
    "    \n",
    "    # Format results\n",
    "    results = []\n",
    "    valid_idx = 0\n",
    "    for i, text in enumerate(texts):\n",
    "        if i not in valid_indices:\n",
    "            results.append({\n",
    "                'text': text,\n",
    "                'prediction': -1,\n",
    "                'label': 'Insufficient text for analysis',\n",
    "                'confidence': 0.0,\n",
    "                'depression_probability': 0.0,\n",
    "                'probabilities': {'No depression': 50.0, 'Depression': 50.0}\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        prob = probabilities[valid_idx]\n",
    "        depression_prob = prob[1]  # Probability of depression\n",
    "        \n",
    "        # Use threshold-based approach\n",
    "        if depression_prob >= depression_threshold:\n",
    "            prediction = 1\n",
    "            label = \"Depression detected\"\n",
    "            confidence = depression_prob * 100\n",
    "        else:\n",
    "            prediction = 0\n",
    "            label = \"No depression detected\"\n",
    "            confidence = prob[0] * 100\n",
    "        \n",
    "        is_borderline = abs(depression_prob - 0.5) < 0.15\n",
    "        \n",
    "        results.append({\n",
    "            'text': text,\n",
    "            'prediction': int(prediction),\n",
    "            'label': label,\n",
    "            'confidence': round(confidence, 2),\n",
    "            'depression_probability': round(depression_prob * 100, 2),\n",
    "            'is_borderline': is_borderline,\n",
    "            'probabilities': {\n",
    "                'No depression': round(prob[0] * 100, 2),\n",
    "                'Depression': round(prob[1] * 100, 2)\n",
    "            }\n",
    "        })\n",
    "        valid_idx += 1\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example: Batch prediction\n",
    "sample_tweets = [\n",
    "    \"Feeling really sad and hopeless today\",\n",
    "    \"Great weather today! Going to the park\",\n",
    "    \"I can't find motivation to do anything\",\n",
    "    \"Excited about my new project!\"\n",
    "]\n",
    "\n",
    "batch_results = predict_batch(sample_tweets, depression_threshold=0.40)\n",
    "\n",
    "print(\"=== Batch Prediction Results ===\\n\")\n",
    "for i, result in enumerate(batch_results, 1):\n",
    "    print(f\"{i}. {result['text']}\")\n",
    "    print(f\"    {result['label']}\")\n",
    "    print(f\"   Depression Probability: {result['depression_probability']}%\")\n",
    "    print(f\"   Confidence: {result['confidence']}%\")\n",
    "    if result.get('is_borderline', False):\n",
    "        print(f\"     Borderline case - model is uncertain\")\n",
    "    print(f\"   Full probabilities: {result['probabilities']}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Summary\n",
    "\n",
    "This notebook provides a complete pipeline for mental health detection:\n",
    "\n",
    "1. **Preprocessing**: Text cleaning, normalization, and feature extraction\n",
    "2. **Model Training**: Logistic Regression with TF-IDF features\n",
    "3. **Model Performance**: ~95.6% accuracy on test set\n",
    "4. **Prediction Functions**: \n",
    "   - Single tweet prediction\n",
    "   - Batch prediction\n",
    "   - Probability scores included\n",
    "\n",
    "### Usage:\n",
    "- Use `predict_mental_health(text)` for single predictions\n",
    "- Use `predict_batch(texts)` for multiple predictions\n",
    "- Both functions can work with saved models or accept model/vectorizer as parameters\n",
    "\n",
    "### Model Files:\n",
    "- Model: `../models/mental_health_model.pkl`\n",
    "- Vectorizer: `../models/tfidf_vectorizer.pkl`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
