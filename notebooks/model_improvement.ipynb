{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Improvement: Advanced Techniques for Higher Accuracy\n",
    "\n",
    "This notebook implements several advanced techniques to improve model accuracy:\n",
    "1. Multiple model comparison (Logistic Regression, Random Forest, XGBoost, SVM)\n",
    "2. Hyperparameter tuning with GridSearchCV\n",
    "3. Ensemble methods (Voting Classifier)\n",
    "4. Feature selection\n",
    "5. Cross-validation for robust evaluation\n",
    "6. Better feature engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import pickle\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download NLTK resources\n",
    "try:\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('omw-1.4', quiet=True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if pd.isna(text) or text is None:\n",
    "        return ''\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", '', text)\n",
    "    text = re.sub(r\"<.*?>\", \" \", text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    words = text.split()\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    words = [w for w in words if w not in stop_words]\n",
    "    words = [lemmatizer.lemmatize(w) for w in words]\n",
    "    return ' '.join(words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Preprocess Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Data Visualization - Label Distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize label distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Count plot\n",
    "label_counts = df['is_depression'].value_counts()\n",
    "axes[0].bar(['No Depression', 'Depression'], label_counts.values, color=['#4caf50', '#f44336'], alpha=0.7)\n",
    "axes[0].set_title('Label Distribution (Count)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "axes[0].set_xlabel('Label', fontsize=12)\n",
    "for i, v in enumerate(label_counts.values):\n",
    "    axes[0].text(i, v + 50, str(v), ha='center', fontweight='bold', fontsize=12)\n",
    "\n",
    "# Pie chart\n",
    "label_props = df['is_depression'].value_counts(normalize=True)\n",
    "colors = ['#4caf50', '#f44336']\n",
    "axes[1].pie(label_props.values, labels=['No Depression', 'Depression'], autopct='%1.1f%%', \n",
    "            colors=colors, startangle=90, textprops={'fontsize': 12, 'fontweight': 'bold'})\n",
    "axes[1].set_title('Label Distribution (Percentage)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Dataset is {'balanced' if abs(label_props[0] - label_props[1]) < 0.1 else 'imbalanced'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Text Length Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate text lengths\n",
    "df['text_length'] = df['clean_text'].apply(lambda x: len(str(x).split()))\n",
    "df['char_length'] = df['clean_text'].apply(lambda x: len(str(x)))\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Word count distribution by label\n",
    "for label in [0, 1]:\n",
    "    label_name = 'No Depression' if label == 0 else 'Depression'\n",
    "    data = df[df['is_depression'] == label]['text_length']\n",
    "    axes[0, 0].hist(data, bins=50, alpha=0.6, label=label_name, \n",
    "                    color='#4caf50' if label == 0 else '#f44336', edgecolor='black')\n",
    "axes[0, 0].set_xlabel('Word Count', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0, 0].set_title('Word Count Distribution by Label', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Box plot comparison\n",
    "df_box = df[['is_depression', 'text_length']].copy()\n",
    "df_box['is_depression'] = df_box['is_depression'].map({0: 'No Depression', 1: 'Depression'})\n",
    "sns.boxplot(data=df_box, x='is_depression', y='text_length', ax=axes[0, 1], \n",
    "            palette=['#4caf50', '#f44336'])\n",
    "axes[0, 1].set_title('Word Count Distribution (Box Plot)', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Label', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Word Count', fontsize=12)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Average text length by label\n",
    "avg_lengths = df.groupby('is_depression')['text_length'].mean()\n",
    "bars = axes[1, 0].bar(['No Depression', 'Depression'], avg_lengths.values, \n",
    "                      color=['#4caf50', '#f44336'], alpha=0.7, edgecolor='black')\n",
    "axes[1, 0].set_title('Average Word Count by Label', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Average Word Count', fontsize=12)\n",
    "axes[1, 0].set_xlabel('Label', fontsize=12)\n",
    "for i, v in enumerate(avg_lengths.values):\n",
    "    axes[1, 0].text(i, v + 2, f'{v:.1f}', ha='center', fontweight='bold', fontsize=12)\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 4. Character length distribution\n",
    "for label in [0, 1]:\n",
    "    label_name = 'No Depression' if label == 0 else 'Depression'\n",
    "    data = df[df['is_depression'] == label]['char_length']\n",
    "    axes[1, 1].hist(data, bins=50, alpha=0.6, label=label_name,\n",
    "                    color='#4caf50' if label == 0 else '#f44336', edgecolor='black')\n",
    "axes[1, 1].set_xlabel('Character Count', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1, 1].set_title('Character Count Distribution by Label', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== Text Length Statistics ===\")\n",
    "print(f\"\\nOverall Statistics:\")\n",
    "print(df['text_length'].describe())\n",
    "print(f\"\\nBy Label:\")\n",
    "print(df.groupby('is_depression')['text_length'].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Word Frequency Analysis & Word Clouds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze most common words by label\n",
    "def get_top_words(texts, n=20):\n",
    "    \"\"\"Get top N words from a list of texts\"\"\"\n",
    "    all_words = []\n",
    "    for text in texts:\n",
    "        words = str(text).lower().split()\n",
    "        all_words.extend(words)\n",
    "    word_freq = Counter(all_words)\n",
    "    return word_freq.most_common(n)\n",
    "\n",
    "# Get top words for each class\n",
    "depression_texts = df[df['is_depression'] == 1]['processed_text']\n",
    "no_depression_texts = df[df['is_depression'] == 0]['processed_text']\n",
    "\n",
    "top_depression_words = get_top_words(depression_texts, 20)\n",
    "top_no_depression_words = get_top_words(no_depression_texts, 20)\n",
    "\n",
    "# Visualize top words\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Depression words\n",
    "dep_words, dep_counts = zip(*top_depression_words)\n",
    "axes[0].barh(range(len(dep_words)), dep_counts, color='#f44336', alpha=0.7)\n",
    "axes[0].set_yticks(range(len(dep_words)))\n",
    "axes[0].set_yticklabels(dep_words, fontsize=10)\n",
    "axes[0].set_xlabel('Frequency', fontsize=12)\n",
    "axes[0].set_title('Top 20 Words - Depression Class', fontsize=14, fontweight='bold')\n",
    "axes[0].invert_yaxis()\n",
    "axes[0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# No Depression words\n",
    "no_dep_words, no_dep_counts = zip(*top_no_depression_words)\n",
    "axes[1].barh(range(len(no_dep_words)), no_dep_counts, color='#4caf50', alpha=0.7)\n",
    "axes[1].set_yticks(range(len(no_dep_words)))\n",
    "axes[1].set_yticklabels(no_dep_words, fontsize=10)\n",
    "axes[1].set_xlabel('Frequency', fontsize=12)\n",
    "axes[1].set_title('Top 20 Words - No Depression Class', fontsize=14, fontweight='bold')\n",
    "axes[1].invert_yaxis()\n",
    "axes[1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Word Clouds\n",
    "print(\"\\nGenerating Word Clouds...\")\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# Depression word cloud\n",
    "depression_text = ' '.join(depression_texts.astype(str))\n",
    "wordcloud_dep = WordCloud(width=800, height=400, background_color='white', \n",
    "                          colormap='Reds', max_words=100).generate(depression_text)\n",
    "axes[0].imshow(wordcloud_dep, interpolation='bilinear')\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title('Word Cloud - Depression Class', fontsize=16, fontweight='bold', pad=20)\n",
    "\n",
    "# No Depression word cloud\n",
    "no_depression_text = ' '.join(no_depression_texts.astype(str))\n",
    "wordcloud_no_dep = WordCloud(width=800, height=400, background_color='white', \n",
    "                            colormap='Greens', max_words=100).generate(no_depression_text)\n",
    "axes[1].imshow(wordcloud_no_dep, interpolation='bilinear')\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title('Word Cloud - No Depression Class', fontsize=16, fontweight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (7730, 4)\n",
      "Label distribution:\n",
      "is_depression\n",
      "0    3900\n",
      "1    3830\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Set base directory\n",
    "base_dir = 'D:/mental_health_detector'\n",
    "\n",
    "# Load data\n",
    "data_path = os.path.join(base_dir, 'data/processed/depression_dataset_processed.csv')\n",
    "raw_data_path = os.path.join(base_dir, 'data/raw/depression_dataset_reddit_cleaned.csv')\n",
    "\n",
    "if os.path.exists(data_path):\n",
    "    df = pd.read_csv(data_path)\n",
    "    if 'processed_text' not in df.columns:\n",
    "        df['processed_text'] = df['clean_text'].apply(preprocess_text)\n",
    "else:\n",
    "    df = pd.read_csv(raw_data_path)\n",
    "    df['processed_text'] = df['clean_text'].apply(preprocess_text)\n",
    "\n",
    "# Clean data\n",
    "df = df[df['processed_text'].notna() & (df['processed_text'].str.strip() != '')]\n",
    "df = df[df['is_depression'].notna()]\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Label distribution:\")\n",
    "print(df['is_depression'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Enhanced Feature Engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1. Model Comparison Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison results\n",
    "if 'results' in locals() and len(results) > 0:\n",
    "    model_names = list(results.keys())\n",
    "    cv_scores = [results[name]['mean_score'] for name in model_names]\n",
    "    cv_stds = [results[name]['std_score'] for name in model_names]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    bars = ax.barh(model_names, cv_scores, xerr=cv_stds, color=['#2196F3', '#4caf50', '#ff9800', '#9c27b0', '#e91e63', '#00bcd4'], \n",
    "                   alpha=0.7, edgecolor='black', capsize=5)\n",
    "    ax.set_xlabel('Cross-Validation Accuracy', fontsize=12)\n",
    "    ax.set_title('Model Comparison (5-Fold CV)', fontsize=16, fontweight='bold')\n",
    "    ax.set_xlim([0.85, 1.0])\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (name, score, std) in enumerate(zip(model_names, cv_scores, cv_stds)):\n",
    "        ax.text(score + std + 0.005, i, f'{score:.4f} (±{std:.4f})', \n",
    "               va='center', fontweight='bold', fontsize=10)\n",
    "    \n",
    "    # Highlight best model\n",
    "    best_idx = np.argmax(cv_scores)\n",
    "    bars[best_idx].set_color('#f44336')\n",
    "    bars[best_idx].set_edgecolor('black')\n",
    "    bars[best_idx].set_linewidth(3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n=== Model Comparison Summary ===\")\n",
    "    for name in model_names:\n",
    "        print(f\"{name}: {results[name]['mean_score']:.4f} (±{results[name]['std_score']:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (7730, 8000)\n",
      "Number of features: 8000\n"
     ]
    }
   ],
   "source": [
    "# Enhanced TF-IDF with better parameters\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=8000,  # Increased from 5000\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,  # Ignore terms that appear in less than 2 documents\n",
    "    max_df=0.95,  # Ignore terms that appear in more than 95% of documents\n",
    "    sublinear_tf=True  # Apply sublinear tf scaling (1 + log(tf))\n",
    ")\n",
    "\n",
    "processed_texts = df['processed_text'].fillna('').astype(str).tolist()\n",
    "X_tfidf = vectorizer.fit_transform(processed_texts)\n",
    "y = df['is_depression'].values\n",
    "\n",
    "print(f\"Feature matrix shape: {X_tfidf.shape}\")\n",
    "print(f\"Number of features: {X_tfidf.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Selection (Optional - can improve accuracy by removing noise)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original features: 8000\n",
      "Selected features: 6000\n"
     ]
    }
   ],
   "source": [
    "# Feature selection - select top k features using chi2\n",
    "# This can help remove noise and improve accuracy\n",
    "k_best = 6000  # Select top 6000 features\n",
    "selector = SelectKBest(chi2, k=min(k_best, X_tfidf.shape[1]))\n",
    "X_selected = selector.fit_transform(X_tfidf, y)\n",
    "\n",
    "print(f\"Original features: {X_tfidf.shape[1]}\")\n",
    "print(f\"Selected features: {X_selected.shape[1]}\")\n",
    "\n",
    "# Use selected features\n",
    "X = X_selected\n",
    "# Or use all features: X = X_tfidf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train-Test Split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1. Final Model Performance Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive performance visualizations for best model\n",
    "best_model_name = max(final_results, key=lambda x: final_results[x]['accuracy'])\n",
    "best_result = final_results[best_model_name]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Confusion Matrix\n",
    "y_pred_best = best_result['model'].predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 0],\n",
    "            xticklabels=['No Depression', 'Depression'],\n",
    "            yticklabels=['No Depression', 'Depression'],\n",
    "            cbar_kws={'label': 'Count'})\n",
    "axes[0, 0].set_title(f'Confusion Matrix - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('True Label', fontsize=12)\n",
    "axes[0, 0].set_xlabel('Predicted Label', fontsize=12)\n",
    "\n",
    "# Add percentages\n",
    "cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        axes[0, 0].text(j+0.5, i+0.7, f'{cm_percent[i, j]:.1f}%', \n",
    "                       ha='center', va='center', fontsize=10, color='red', fontweight='bold')\n",
    "\n",
    "# 2. ROC Curve\n",
    "y_pred_proba_best = best_result['model'].predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba_best)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba_best)\n",
    "\n",
    "axes[0, 1].plot(fpr, tpr, color='#2196F3', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "axes[0, 1].plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--', label='Random Classifier')\n",
    "axes[0, 1].set_xlim([0.0, 1.0])\n",
    "axes[0, 1].set_ylim([0.0, 1.05])\n",
    "axes[0, 1].set_xlabel('False Positive Rate', fontsize=12)\n",
    "axes[0, 1].set_ylabel('True Positive Rate', fontsize=12)\n",
    "axes[0, 1].set_title(f'ROC Curve - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].legend(loc=\"lower right\")\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Metrics Comparison\n",
    "metrics = {\n",
    "    'Accuracy': best_result['accuracy'],\n",
    "    'Precision': best_result['precision'],\n",
    "    'Recall': best_result['recall'],\n",
    "    'F1-Score': best_result['f1']\n",
    "}\n",
    "bars = axes[1, 0].bar(metrics.keys(), metrics.values(), \n",
    "                     color=['#4caf50', '#2196F3', '#ff9800', '#9c27b0'], alpha=0.7, edgecolor='black')\n",
    "axes[1, 0].set_ylim([0, 1])\n",
    "axes[1, 0].set_ylabel('Score', fontsize=12)\n",
    "axes[1, 0].set_title(f'Performance Metrics - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "for i, (key, value) in enumerate(metrics.items()):\n",
    "    axes[1, 0].text(i, value + 0.02, f'{value:.3f}', ha='center', fontweight='bold', fontsize=11)\n",
    "\n",
    "# 4. Probability Distribution\n",
    "y_pred_proba_depression = y_pred_proba_best\n",
    "y_pred_proba_no_depression = 1 - y_pred_proba_best\n",
    "\n",
    "axes[1, 1].hist(y_pred_proba_no_depression[y_test == 0], bins=30, alpha=0.6, \n",
    "                label='No Depression (True)', color='#4caf50', edgecolor='black')\n",
    "axes[1, 1].hist(y_pred_proba_depression[y_test == 1], bins=30, alpha=0.6, \n",
    "                label='Depression (True)', color='#f44336', edgecolor='black')\n",
    "axes[1, 1].axvline(x=0.5, color='black', linestyle='--', linewidth=2, label='Decision Threshold (0.5)')\n",
    "axes[1, 1].set_xlabel('Predicted Probability', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1, 1].set_title('Prediction Probability Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n=== {best_model_name} Performance Summary ===\")\n",
    "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
    "print(f\"Accuracy: {best_result['accuracy']:.4f}\")\n",
    "print(f\"Precision: {best_result['precision']:.4f}\")\n",
    "print(f\"Recall: {best_result['recall']:.4f}\")\n",
    "print(f\"F1-Score: {best_result['f1']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2. All Models Performance Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all final models side by side\n",
    "if len(final_results) > 1:\n",
    "    model_names = list(final_results.keys())\n",
    "    accuracies = [final_results[name]['accuracy'] for name in model_names]\n",
    "    precisions = [final_results[name]['precision'] for name in model_names]\n",
    "    recalls = [final_results[name]['recall'] for name in model_names]\n",
    "    f1_scores = [final_results[name]['f1'] for name in model_names]\n",
    "    \n",
    "    x = np.arange(len(model_names))\n",
    "    width = 0.2\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    bars1 = ax.bar(x - 1.5*width, accuracies, width, label='Accuracy', color='#4caf50', alpha=0.7, edgecolor='black')\n",
    "    bars2 = ax.bar(x - 0.5*width, precisions, width, label='Precision', color='#2196F3', alpha=0.7, edgecolor='black')\n",
    "    bars3 = ax.bar(x + 0.5*width, recalls, width, label='Recall', color='#ff9800', alpha=0.7, edgecolor='black')\n",
    "    bars4 = ax.bar(x + 1.5*width, f1_scores, width, label='F1-Score', color='#9c27b0', alpha=0.7, edgecolor='black')\n",
    "    \n",
    "    ax.set_ylabel('Score', fontsize=12)\n",
    "    ax.set_title('Model Performance Comparison', fontsize=16, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(model_names, fontsize=11)\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.set_ylim([0, 1.05])\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bars in [bars1, bars2, bars3, bars4]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                   f'{height:.3f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Create comparison table\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Model': model_names,\n",
    "        'Accuracy': accuracies,\n",
    "        'Precision': precisions,\n",
    "        'Recall': recalls,\n",
    "        'F1-Score': f1_scores\n",
    "    })\n",
    "    print(\"\\n=== Model Performance Comparison Table ===\")\n",
    "    print(comparison_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 6184\n",
      "Test samples: 1546\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Test samples: {X_test.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Comparison - Test Multiple Algorithms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating models with 5-fold cross-validation...\n",
      "\n",
      "Logistic Regression: 0.9531 (+/- 0.0141)\n",
      "Random Forest: 0.8984 (+/- 0.0144)\n",
      "XGBoost: 0.9544 (+/- 0.0125)\n",
      "SVM: 0.9559 (+/- 0.0149)\n",
      "Gradient Boosting: 0.9486 (+/- 0.0180)\n",
      "Naive Bayes: 0.9125 (+/- 0.0197)\n",
      "\n",
      "Best model: SVM with CV accuracy: 0.9559\n"
     ]
    }
   ],
   "source": [
    "# Define models to test\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=2000, random_state=42, class_weight='balanced'),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=200, max_depth=20, random_state=42, class_weight='balanced', n_jobs=-1),\n",
    "    'XGBoost': xgb.XGBClassifier(n_estimators=200, max_depth=6, learning_rate=0.1, random_state=42, eval_metric='logloss'),\n",
    "    'SVM': SVC(kernel='linear', probability=True, random_state=42, class_weight='balanced'),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=200, max_depth=5, learning_rate=0.1, random_state=42),\n",
    "    'Naive Bayes': MultinomialNB(alpha=0.1)\n",
    "}\n",
    "\n",
    "# Evaluate each model with cross-validation\n",
    "results = {}\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"Evaluating models with 5-fold cross-validation...\\n\")\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "    results[name] = {\n",
    "        'mean_score': scores.mean(),\n",
    "        'std_score': scores.std(),\n",
    "        'model': model\n",
    "    }\n",
    "    print(f\"{name}: {scores.mean():.4f} (+/- {scores.std()*2:.4f})\")\n",
    "\n",
    "# Find best model\n",
    "best_model_name = max(results, key=lambda x: results[x]['mean_score'])\n",
    "print(f\"\\nBest model: {best_model_name} with CV accuracy: {results[best_model_name]['mean_score']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Hyperparameter Tuning for Best Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing hyperparameter tuning for XGBoost...\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "\n",
      "Best parameters: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 300, 'subsample': 1.0}\n",
      "Best CV score: 0.9546\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning for XGBoost (usually performs best)\n",
    "print(\"Performing hyperparameter tuning for XGBoost...\")\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [200, 300],\n",
    "    'max_depth': [5, 6, 7],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "xgb_base = xgb.XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "grid_search = GridSearchCV(\n",
    "    xgb_base, param_grid, cv=3, scoring='accuracy', n_jobs=-1, verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "best_xgb = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Ensemble Model - Voting Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ensemble model...\n",
      "Ensemble CV accuracy: 0.9575 (+/- 0.0120)\n"
     ]
    }
   ],
   "source": [
    "# Create ensemble of best performing models\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('xgb', best_xgb if 'best_xgb' in locals() else xgb.XGBClassifier(n_estimators=200, max_depth=6, learning_rate=0.1, random_state=42, eval_metric='logloss')),\n",
    "        ('rf', RandomForestClassifier(n_estimators=200, max_depth=20, random_state=42, class_weight='balanced', n_jobs=-1)),\n",
    "        ('lr', LogisticRegression(max_iter=2000, random_state=42, class_weight='balanced', C=1.0))\n",
    "    ],\n",
    "    voting='soft',  # Use probability voting\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Training ensemble model...\")\n",
    "ensemble.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate ensemble\n",
    "ensemble_scores = cross_val_score(ensemble, X_train, y_train, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "print(f\"Ensemble CV accuracy: {ensemble_scores.mean():.4f} (+/- {ensemble_scores.std()*2:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Final Model Evaluation on Test Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Best XGBoost Performance ===\n",
      "Accuracy: 0.9599\n",
      "Precision: 0.9770\n",
      "Recall: 0.9413\n",
      "F1-Score: 0.9588\n",
      "\n",
      "Confusion Matrix:\n",
      "[[763  17]\n",
      " [ 45 721]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.98      0.96       780\n",
      "           1       0.98      0.94      0.96       766\n",
      "\n",
      "    accuracy                           0.96      1546\n",
      "   macro avg       0.96      0.96      0.96      1546\n",
      "weighted avg       0.96      0.96      0.96      1546\n",
      "\n",
      "\n",
      "=== Ensemble Performance ===\n",
      "Accuracy: 0.9638\n",
      "Precision: 0.9876\n",
      "Recall: 0.9386\n",
      "F1-Score: 0.9625\n",
      "\n",
      "Confusion Matrix:\n",
      "[[771   9]\n",
      " [ 47 719]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.96       780\n",
      "           1       0.99      0.94      0.96       766\n",
      "\n",
      "    accuracy                           0.96      1546\n",
      "   macro avg       0.97      0.96      0.96      1546\n",
      "weighted avg       0.96      0.96      0.96      1546\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train final models and evaluate on test set\n",
    "final_models = {\n",
    "    'Best XGBoost': best_xgb if 'best_xgb' in locals() else xgb.XGBClassifier(n_estimators=300, max_depth=6, learning_rate=0.1, random_state=42, eval_metric='logloss'),\n",
    "    'Ensemble': ensemble\n",
    "}\n",
    "\n",
    "final_results = {}\n",
    "\n",
    "for name, model in final_models.items():\n",
    "    # Train\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
    "    \n",
    "    final_results[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'model': model\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n=== {name} Performance ===\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Best Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: Ensemble\n",
      "Accuracy: 0.9638\n",
      "\n",
      "Model saved to D:/mental_health_detector\\models\\mental_health_model_improved.pkl\n",
      "Vectorizer saved to D:/mental_health_detector\\models\\tfidf_vectorizer_improved.pkl\n",
      "Feature selector saved to D:/mental_health_detector\\models\\feature_selector.pkl\n"
     ]
    }
   ],
   "source": [
    "# Find best model\n",
    "best_final_name = max(final_results, key=lambda x: final_results[x]['accuracy'])\n",
    "best_final_model = final_results[best_final_name]['model']\n",
    "\n",
    "print(f\"Best model: {best_final_name}\")\n",
    "print(f\"Accuracy: {final_results[best_final_name]['accuracy']:.4f}\")\n",
    "\n",
    "# Save model and vectorizer\n",
    "models_dir = os.path.join(base_dir, 'models')\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Save improved model\n",
    "model_path = os.path.join(models_dir, 'mental_health_model_improved.pkl')\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(best_final_model, f)\n",
    "print(f\"\\nModel saved to {model_path}\")\n",
    "\n",
    "# Save vectorizer\n",
    "vectorizer_path = os.path.join(models_dir, 'tfidf_vectorizer_improved.pkl')\n",
    "with open(vectorizer_path, 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "print(f\"Vectorizer saved to {vectorizer_path}\")\n",
    "\n",
    "# Save selector if used\n",
    "if 'selector' in locals():\n",
    "    selector_path = os.path.join(models_dir, 'feature_selector.pkl')\n",
    "    with open(selector_path, 'wb') as f:\n",
    "        pickle.dump(selector, f)\n",
    "    print(f\"Feature selector saved to {selector_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
